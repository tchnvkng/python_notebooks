{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aec69c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class DifferentialDNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims=[64, 64], alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        layers = []\n",
    "        dims = [input_dim] + hidden_dims\n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(dims[-1], output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def compute_jacobian(self, outputs, inputs, n):\n",
    "        \"\"\"\n",
    "        outputs: (batch, p)\n",
    "        inputs: (batch, n + m), requires_grad=True\n",
    "        Returns: (batch, p, n)\n",
    "        \"\"\"\n",
    "        inputs_n = inputs[:, :n]\n",
    "        batch_size, p = outputs.shape\n",
    "        jac = []\n",
    "        for i in range(p):\n",
    "            grad = torch.autograd.grad(\n",
    "                outputs[:, i], inputs_n,\n",
    "                grad_outputs=torch.ones_like(outputs[:, i]),\n",
    "                retain_graph=True, create_graph=True,\n",
    "                allow_unused=True\n",
    "            )[0]\n",
    "            if grad is None:\n",
    "                grad = torch.zeros_like(inputs_n)\n",
    "            jac.append(grad.unsqueeze(1))  # (batch, 1, n)\n",
    "        return torch.cat(jac, dim=1)  # (batch, p, n)\n",
    "\n",
    "    def differential_loss(self, y_pred, y_true, j_pred, j_true):\n",
    "        value_loss = F.mse_loss(y_pred, y_true)\n",
    "        jacobian_loss = F.mse_loss(j_pred, j_true)\n",
    "        return self.alpha * value_loss + (1 - self.alpha) * jacobian_loss\n",
    "\n",
    "    def fit(self, x, y, j, n, epochs=100, batch_size=64, lr=1e-3):\n",
    "        self.train()\n",
    "        dataset = TensorDataset(x, y, j)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0.0\n",
    "            for xb, yb, jb in loader:\n",
    "                xb = xb.requires_grad_()  # important for autograd\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = self.forward(xb)\n",
    "                j_pred = self.compute_jacobian(y_pred, xb, n)\n",
    "                loss = self.differential_loss(y_pred, yb, j_pred, jb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item() * xb.size(0)\n",
    "            avg_loss = total_loss / len(dataset)\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "\n",
    "    def predict(self, x, n):\n",
    "        self.eval()\n",
    "        x = x.clone().detach().requires_grad_()\n",
    "        y_pred = self.forward(x)\n",
    "        j_pred = self.compute_jacobian(y_pred, x, n)\n",
    "        return y_pred.detach(), j_pred.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb201ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.332743\n",
      "Epoch 2/5, Loss: 0.309249\n",
      "Epoch 3/5, Loss: 0.284406\n",
      "Epoch 4/5, Loss: 0.254743\n",
      "Epoch 5/5, Loss: 0.224271\n"
     ]
    }
   ],
   "source": [
    "# Dimensions\n",
    "N, n, m, p = 500, 24, 5, 10\n",
    "\n",
    "# Synthetic data where outputs depend on differentiable inputs\n",
    "x = torch.randn(N, n + m)\n",
    "y = torch.sin(x[:, :p]) + 0.1 * torch.randn(N, p)\n",
    "j = torch.zeros(N, p, n)\n",
    "for i in range(p):\n",
    "    for j_idx in range(n):\n",
    "        j[:, i, j_idx] = torch.cos(x[:, j_idx]) if j_idx < p else 0.0\n",
    "\n",
    "model = DifferentialDNN(input_dim=n + m, output_dim=p, alpha=0.5)\n",
    "model.fit(x, y, j, n=n, epochs=5, batch_size=32)\n",
    "\n",
    "x_test = torch.randn(10, n + m)\n",
    "y_pred, j_pred = model.predict(x_test, n=n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b5371a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class DifferentialDNN2(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims=[64, 64], alpha=0.5, scaling=True):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.scaling = scaling\n",
    "\n",
    "        layers = []\n",
    "        dims = [input_dim] + hidden_dims\n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(dims[-1], output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "        # Will be set during fit()\n",
    "        self.n = None\n",
    "        self.input_mean = None\n",
    "        self.input_std = None\n",
    "        self.output_mean = None\n",
    "        self.output_std = None\n",
    "        self.jacobian_scale = None  # To scale jacobian rows appropriately\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    # --- Scaling methods ---\n",
    "\n",
    "    def _scale_inputs(self, x):\n",
    "        if not self.scaling or self.input_mean is None:\n",
    "            return x\n",
    "        return (x - self.input_mean) / self.input_std\n",
    "\n",
    "    def _unscale_outputs(self, y_scaled):\n",
    "        if not self.scaling or self.output_mean is None:\n",
    "            return y_scaled\n",
    "        return y_scaled * self.output_std + self.output_mean\n",
    "\n",
    "    def _scale_jacobians(self, j_raw):\n",
    "        if not self.scaling or self.input_std is None or self.output_std is None:\n",
    "            return j_raw\n",
    "        input_std_n = self.input_std[:self.n]  # only first n inputs\n",
    "        scale = (input_std_n.view(1, 1, -1) / self.output_std.view(1, -1, 1))\n",
    "        return j_raw * scale\n",
    "\n",
    "    def _unscale_jacobians(self, j_scaled):\n",
    "        if not self.scaling or self.input_std is None or self.output_std is None:\n",
    "            return j_scaled\n",
    "        input_std_n = self.input_std[:self.n]\n",
    "        scale = (self.output_std.view(-1, 1) / input_std_n.view(1, -1))\n",
    "        return j_scaled * scale\n",
    "\n",
    "    # --- Jacobian computation with allow_unused fix ---\n",
    "\n",
    "    def compute_jacobian(self, outputs, inputs, n):\n",
    "        inputs_n = inputs[:, :n]\n",
    "        batch_size, p = outputs.shape\n",
    "        jac = []\n",
    "        for i in range(p):\n",
    "            grad = torch.autograd.grad(\n",
    "                outputs[:, i], inputs_n,\n",
    "                grad_outputs=torch.ones_like(outputs[:, i]),\n",
    "                retain_graph=True, create_graph=True,\n",
    "                allow_unused=True\n",
    "            )[0]\n",
    "            if grad is None:\n",
    "                grad = torch.zeros_like(inputs_n)\n",
    "            jac.append(grad.unsqueeze(1))\n",
    "        return torch.cat(jac, dim=1)\n",
    "\n",
    "    def differential_loss(self, y_pred, y_true, j_pred, j_true):\n",
    "        value_loss = F.mse_loss(y_pred, y_true)\n",
    "        jacobian_loss = F.mse_loss(j_pred, j_true)\n",
    "        return self.alpha * value_loss + (1 - self.alpha) * jacobian_loss\n",
    "\n",
    "    # --- Fit and Predict ---\n",
    "\n",
    "    def fit(self, x, y, j, n, epochs=100, batch_size=64, lr=1e-3):\n",
    "        \"\"\"\n",
    "        x: (N, n + m) raw inputs\n",
    "        y: (N, p) raw outputs\n",
    "        j: (N, p, n) raw Jacobians\n",
    "        n: number of differentiable inputs\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        self.n = n\n",
    "\n",
    "        # Compute scaling params if scaling enabled\n",
    "        if self.scaling:\n",
    "            self.input_mean = x.mean(dim=0, keepdim=True)\n",
    "            self.input_std = x.std(dim=0, unbiased=False, keepdim=True).clamp(min=1e-6)\n",
    "            self.output_mean = y.mean(dim=0, keepdim=True)\n",
    "            self.output_std = y.std(dim=0, unbiased=False, keepdim=True).clamp(min=1e-6)\n",
    "\n",
    "            # flatten for jacobian scaling (p, n)\n",
    "            self.input_std = self.input_std.view(-1)\n",
    "            self.output_std = self.output_std.view(-1)\n",
    "\n",
    "            # scale training data\n",
    "            x_scaled = self._scale_inputs(x)\n",
    "            y_scaled = (y - self.output_mean) / self.output_std\n",
    "            j_scaled = self._scale_jacobians(j)\n",
    "        else:\n",
    "            x_scaled, y_scaled, j_scaled = x, y, j\n",
    "\n",
    "        dataset = TensorDataset(x_scaled, y_scaled, j_scaled)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0.0\n",
    "            for xb, yb, jb in loader:\n",
    "                xb = xb.requires_grad_()\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = self.forward(xb)\n",
    "                j_pred = self.compute_jacobian(y_pred, xb, n)\n",
    "                loss = self.differential_loss(y_pred, yb, j_pred, jb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item() * xb.size(0)\n",
    "            avg_loss = total_loss / len(dataset)\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "\n",
    "    def predict(self, x, n):\n",
    "        \"\"\"\n",
    "        x: raw inputs (batch, n+m)\n",
    "        returns: unscaled outputs and jacobians\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        # scale inputs if needed\n",
    "        x_scaled = self._scale_inputs(x) if self.scaling else x\n",
    "        x_scaled = x_scaled.clone().detach().requires_grad_()\n",
    "        y_pred = self.forward(x_scaled)\n",
    "        j_pred = self.compute_jacobian(y_pred, x_scaled, n)\n",
    "\n",
    "        # unscale outputs and jacobians\n",
    "        y_pred_unscaled = self._unscale_outputs(y_pred) if self.scaling else y_pred\n",
    "        j_pred_unscaled = self._unscale_jacobians(j_pred) if self.scaling else j_pred\n",
    "\n",
    "        return y_pred_unscaled.detach(), j_pred_unscaled.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cd55410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.004140\n",
      "Epoch 2/20, Loss: 0.994670\n",
      "Epoch 3/20, Loss: 0.989822\n",
      "Epoch 4/20, Loss: 0.985094\n",
      "Epoch 5/20, Loss: 0.980353\n",
      "Epoch 6/20, Loss: 0.975027\n",
      "Epoch 7/20, Loss: 0.969598\n",
      "Epoch 8/20, Loss: 0.963767\n",
      "Epoch 9/20, Loss: 0.957729\n",
      "Epoch 10/20, Loss: 0.951760\n",
      "Epoch 11/20, Loss: 0.945812\n",
      "Epoch 12/20, Loss: 0.939422\n",
      "Epoch 13/20, Loss: 0.932464\n",
      "Epoch 14/20, Loss: 0.926181\n",
      "Epoch 15/20, Loss: 0.919883\n",
      "Epoch 16/20, Loss: 0.912709\n",
      "Epoch 17/20, Loss: 0.906905\n",
      "Epoch 18/20, Loss: 0.900491\n",
      "Epoch 19/20, Loss: 0.893652\n",
      "Epoch 20/20, Loss: 0.886201\n"
     ]
    }
   ],
   "source": [
    "n, m, p = 24, 5, 10\n",
    "N = 500\n",
    "x = torch.randn(N, n + m)\n",
    "y = torch.randn(N, p)\n",
    "j = torch.randn(N, p, n)\n",
    "\n",
    "model = DifferentialDNN2(input_dim=n + m, output_dim=p, alpha=0.5, scaling=True)\n",
    "model.fit(x, y, j, n=n, epochs=20, batch_size=32)\n",
    "\n",
    "x_test = torch.randn(10, n + m)\n",
    "y_pred, j_pred = model.predict(x_test, n=n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbce527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
