{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e07eb1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manuel/github_mnlbllstr/python_notebooks/.venv/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b6e78",
   "metadata": {},
   "source": [
    "Inputs at time t:  \n",
    "  $N_{t-1}$ (prob vector, length $m$)  \n",
    "  $y_t$ (vector, length $m$)  \n",
    "  $r_t$ (vector, length $m$)  \n",
    "  $g_{t-1}$ (vector, length $m$)  \n",
    "\n",
    " Concatenate all  vector of length $4m$\n",
    "\n",
    "\n",
    "| MLP (2 layers)        | \n",
    "|-----------------------|\n",
    "| Input dim: 4m         |\n",
    "| Hidden layers: e.g.128 |\n",
    "| Output dim: m         |\n",
    "| Outputs: Î” logits     |\n",
    "| (delta_logits)        |\n",
    "\n",
    "\n",
    "\n",
    "Previous logits:  \n",
    "$$\n",
    "\\ell_{t-1} = \\log(N_{t-1} + \\varepsilon)\n",
    "$$\n",
    "\n",
    "Add residual:  \n",
    "$$\n",
    "\\ell_t = \\ell_{t-1} + \\Delta \\text{logits}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Softmax}(\\ell_t) \\to N_t  \\quad \\text{(probability distribution at time t)}\n",
    "$$\n",
    "\n",
    " \\( N_t \\) is then used in:  \n",
    "  - \\( g_t \\) update:  \n",
    "  $$\n",
    "  g_t = \\alpha \\odot g_{t-1} + (N_t - \\alpha \\odot N_{t-1}) \\odot y_t\n",
    "  $$\n",
    "  - next time step input \\( N_{t} \\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbf52958",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# F network: predicts changes in logits (residuals) instead of full logits\n",
    "# ----------------------------------------------------------------------\n",
    "class FNetResidual(nn.Module):\n",
    "    def __init__(self, m, hidden=128):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(4 * m, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.out = nn.Linear(hidden, m)  # delta logits\n",
    "\n",
    "    def forward(self, N_prev, y_t, r_t, g_prev):\n",
    "        # Concatenate all inputs\n",
    "        x = torch.cat([N_prev, y_t, r_t, g_prev], dim=-1)\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        delta_logits = self.out(h)\n",
    "\n",
    "        # Previous logits from probabilities\n",
    "        prev_logits = torch.log(N_prev + 1e-9)\n",
    "        logits = prev_logits + delta_logits\n",
    "\n",
    "        # Softmax to get new probability vector\n",
    "        N_t = F.softmax(logits, dim=-1)\n",
    "        return N_t, logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caabb29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Full dynamics model with fixed alpha and residual logits\n",
    "# ----------------------------------------------------------------------\n",
    "class DynamicsModelResidual(nn.Module):\n",
    "    def __init__(self, m, alpha_values, hidden=128):\n",
    "        super().__init__()\n",
    "        self.F = FNetResidual(m, hidden)\n",
    "        # Fixed alpha, registered as buffer so it moves with the model\n",
    "        self.register_buffer(\"alpha\", torch.tensor(alpha_values, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, N0, g0, ys, rs):\n",
    "        \"\"\"\n",
    "        N0, g0: (batch, m) initial states\n",
    "        ys, rs: (T, batch, m) exogenous inputs\n",
    "        Returns:\n",
    "            Ns: (T, batch, m)\n",
    "            gs: (T, batch, m)\n",
    "            logits_all: (T, batch, m)\n",
    "        \"\"\"\n",
    "        N_prev = N0\n",
    "        g_prev = g0\n",
    "        Ns, gs, logits_all = [], [], []\n",
    "\n",
    "        for t in range(ys.size(0)):\n",
    "            y_t = ys[t]\n",
    "            r_t = rs[t]\n",
    "\n",
    "            # Predict N_t with residual logits\n",
    "            N_t, logits = self.F(N_prev, y_t, r_t, g_prev)\n",
    "\n",
    "            # Elementwise g_t update\n",
    "            g_t = self.alpha * g_prev + (N_t - self.alpha * N_prev) * y_t\n",
    "\n",
    "            # Store\n",
    "            Ns.append(N_t)\n",
    "            gs.append(g_t)\n",
    "            logits_all.append(logits)\n",
    "\n",
    "            # Update for next step\n",
    "            N_prev = N_t\n",
    "            g_prev = g_t\n",
    "\n",
    "        return torch.stack(Ns), torch.stack(gs), torch.stack(logits_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "639c6e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4f/wn1xjjy93pd0rpldf_k_gbsm0000gn/T/ipykernel_97180/1097020467.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"alpha\", torch.tensor(alpha_values, dtype=torch.float32))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Example usage / test run\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "T = 360       # number of time steps\n",
    "m = 3        # distribution size\n",
    "batch = 32\n",
    "alpha_values = torch.linspace(0.1, 0.9, m)  # Example alpha values\n",
    "\n",
    "# Dummy inputs\n",
    "ys = torch.rand(T, batch, m)\n",
    "rs = torch.rand(T, batch, m)\n",
    "N0 = F.softmax(torch.rand(batch, m), dim=-1)\n",
    "g0 = torch.zeros(batch, m)\n",
    "\n",
    "model = DynamicsModelResidual(m, alpha_values, hidden=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3efba2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss=0.3087\n",
      "Step 50: loss=-0.0112\n",
      "Step 100: loss=-0.0411\n",
      "Step 150: loss=-0.0641\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for step in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    Ns, gs, logits_all = model(N0, g0, ys, rs)\n",
    "\n",
    "    # Example path-dependent loss:\n",
    "    entropy = -(Ns * torch.log(Ns + 1e-9)).sum(dim=-1)  # (T, batch)\n",
    "    loss = gs.pow(2).sum(dim=-1).mean() - 0.1 * entropy.mean()\n",
    "\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        print(f\"Step {step}: loss={loss.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
